import csv
import requests
from getpass import getpass
import json
from datetime import datetime
import time

def load_duplicates(file_path):
    merge_data = []
    
    # Detect the delimiter by reading the first line of the file
    with open(file_path, mode='r', encoding='utf-8') as file:
        first_line = file.readline()
        delimiter = ';' if ';' in first_line else ','

    # Now read the file with the detected delimiter
    with open(file_path, mode='r', encoding='utf-8') as file:
        reader = csv.DictReader(file, delimiter=delimiter)
        
        for row in reader:
            # Get the target UUID (merge candidate) if available, else None
            target_uuid = row.get("Merge Candidate", None)
            # Split UUIDs by comma and space to handle lists like "id1, id2, id3"
            duplicate_uuids = row.get("UUIDs", "").split(", ")
            merge_data.append({
                "target": target_uuid,
                "duplicates": duplicate_uuids
            })
    
    return merge_data

def warn_user(merge_data):
    total_merges = len(merge_data)
    total_duplicates = sum(len(item["duplicates"]) for item in merge_data)
    print(f"Warning: This operation will merge {total_duplicates} UUID(s) into {total_merges} target(s).")
    proceed = input("Do you want to continue? (yes/no): ").strip().lower()
    return proceed == "yes"

def perform_merge(base_url, api_key, merge_data, log_file):
    headers = {
        "accept": "application/json",
        "api-key": api_key,
        "content-type": "application/json"
    }

    with open(log_file, mode='a', encoding='utf-8') as log:
        log.write(f"Merge operation started at {datetime.now()}\n")

        for item in merge_data:
            # Use the Merge Candidate UUID if available; otherwise, select the first UUID as the target
            target_uuid = item["target"] if item["target"] else item["duplicates"][0]
            payload = {
                "items": [{"uuid": target_uuid, "systemName": "ExternalOrganization"}]
            }
            
            # Add the rest of the duplicates to the payload, excluding the target
            duplicates = [uuid for uuid in item["duplicates"] if uuid != target_uuid]
            for uuid in duplicates:
                payload["items"].append({"uuid": uuid, "systemName": "ExternalOrganization"})

            # Log the details of the merge operation
            log.write(f"Preparing to merge the following UUIDs into target {target_uuid}:\n")
            log.write(f"Target: {target_uuid}\n")
            log.write(f"Duplicates: {', '.join(duplicates)}\n")

            # Send POST request to merge endpoint with error handling
            try:
                response = requests.post(f"https://{base_url}/ws/api/external-organizations/merge", headers=headers, data=json.dumps(payload))
                
                if response.status_code == 200:
                    log.write(f"SUCCESS: Merged into target {target_uuid} with duplicates {', '.join(duplicates)}\n\n")
                    print(f"Successfully merged UUIDs into target {target_uuid}.")
                else:
                    # Log and continue on error
                    log.write(f"FAILED: Could not merge into target {target_uuid}. Error: {response.status_code}, {response.text}\n\n")
                    print(f"Failed to merge into target {target_uuid}. Error: {response.status_code}, {response.text}")

            except requests.exceptions.RequestException as e:
                # Log any request errors and continue
                log.write(f"ERROR: Request failed for target {target_uuid}. Exception: {e}\n\n")
                print(f"Request failed for target {target_uuid}. Exception: {e}")

            # Delay to avoid hitting the API too hard
            time.sleep(1)  # Adjust delay as needed

        log.write(f"Merge operation ended at {datetime.now()}\n\n")


def main():
    base_url = input("Enter the base URL (e.g., vbn.aau.dk): ")
    api_key = getpass("Enter your API key: ")
    file_path = "duplicate_organizations.csv"  # Fixed file path as generated by the PureAPI_Ex_org_duplicate_finder.py script
    log_file = "merge_log.txt"

    try:
        # Load data from CSV file
        merge_data = load_duplicates(file_path)
        
        # Warn user and proceed based on confirmation
        if not merge_data:
            print("No merge candidates found in the CSV file.")
            return
        
        if warn_user(merge_data):
            perform_merge(base_url, api_key, merge_data, log_file)
            print(f"Merge operation details logged to {log_file}.")
        else:
            print("Merge operation cancelled by user.")
            
    except Exception as err:
        print(f"An error occurred: {err}")

if __name__ == "__main__":
    main()
